{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torchvision.io import read_image\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512]), torch.Size([1, 1, 512]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model = nn.Transformer(batch_first=True)\n",
    "BATCH_SIZE = 1\n",
    "ENC_SEQ_LEN = 10\n",
    "DEC_SEQ_LEN = 1\n",
    "src = torch.rand((BATCH_SIZE, ENC_SEQ_LEN, transformer_model.d_model))\n",
    "tgt = torch.rand((BATCH_SIZE, DEC_SEQ_LEN, transformer_model.d_model))\n",
    "out = transformer_model(src, tgt)\n",
    "out.flatten(start_dim=1).shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "      (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "      (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "      (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "      (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "      (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "      (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=6, nhead=3, batch_first=True)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "transformer_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 16, 6]),\n",
       " tensor([[[0.4311, 0.3375, 0.3744, 0.2338, 0.7313, 0.2425],\n",
       "          [0.7524, 0.4026, 0.5429, 0.5113, 0.1019, 0.8243],\n",
       "          [0.3046, 0.9968, 0.8439, 0.9041, 0.5958, 0.5893],\n",
       "          [0.6594, 0.0069, 0.8752, 0.9730, 0.9796, 0.9051],\n",
       "          [0.7478, 0.1812, 0.7123, 0.6153, 0.6781, 0.7850],\n",
       "          [0.4912, 0.2456, 0.5649, 0.1205, 0.6635, 0.7606],\n",
       "          [0.2108, 0.1277, 0.7877, 0.7445, 0.0157, 0.1731],\n",
       "          [0.5433, 0.7839, 0.3946, 0.4862, 0.8715, 0.3794],\n",
       "          [0.6247, 0.5759, 0.0515, 0.8373, 0.3619, 0.7004],\n",
       "          [0.6730, 0.1387, 0.6259, 0.9951, 0.7938, 0.3173],\n",
       "          [0.9889, 0.7118, 0.4438, 0.8115, 0.3080, 0.4864],\n",
       "          [0.9882, 0.4976, 0.1106, 0.1627, 0.4282, 0.8043],\n",
       "          [0.3383, 0.9685, 0.9702, 0.5511, 0.5124, 0.4251],\n",
       "          [0.5222, 0.2510, 0.3907, 0.1213, 0.8683, 0.4392],\n",
       "          [0.1580, 0.2315, 0.6607, 0.4778, 0.1598, 0.1549],\n",
       "          [0.5270, 0.9304, 0.2287, 0.6299, 0.5448, 0.3195]]]))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "ENC_SEQ_LEN = 15\n",
    "src = torch.rand(BATCH_SIZE, ENC_SEQ_LEN, encoder_layer.self_attn.embed_dim)\n",
    "out = transformer_encoder(src)\n",
    "ENC_SEQ_LEN = 16\n",
    "src = torch.rand(BATCH_SIZE, ENC_SEQ_LEN, encoder_layer.self_attn.embed_dim)\n",
    "out = transformer_encoder(src)\n",
    "out.shape, src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ang_vel_x</th>\n",
       "      <th>ang_vel_y</th>\n",
       "      <th>ang_vel_z</th>\n",
       "      <th>lin_acc_x</th>\n",
       "      <th>lin_acc_y</th>\n",
       "      <th>lin_acc_z</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-10-29 14:20:16.292325120</th>\n",
       "      <td>1.540823e+09</td>\n",
       "      <td>-0.006392</td>\n",
       "      <td>-0.008522</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>0.052672</td>\n",
       "      <td>-0.158017</td>\n",
       "      <td>9.797023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29 14:20:16.294326016</th>\n",
       "      <td>1.540823e+09</td>\n",
       "      <td>-0.005326</td>\n",
       "      <td>-0.006392</td>\n",
       "      <td>-0.006392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.129286</td>\n",
       "      <td>9.768292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29 14:20:16.296323072</th>\n",
       "      <td>1.540823e+09</td>\n",
       "      <td>-0.010653</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>-0.001065</td>\n",
       "      <td>-0.033519</td>\n",
       "      <td>-0.100556</td>\n",
       "      <td>9.749139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29 14:20:16.298322944</th>\n",
       "      <td>1.540823e+09</td>\n",
       "      <td>-0.009587</td>\n",
       "      <td>-0.009587</td>\n",
       "      <td>-0.002131</td>\n",
       "      <td>0.043095</td>\n",
       "      <td>-0.110133</td>\n",
       "      <td>9.725197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29 14:20:16.300322048</th>\n",
       "      <td>1.540823e+09</td>\n",
       "      <td>-0.005326</td>\n",
       "      <td>-0.005326</td>\n",
       "      <td>-0.004261</td>\n",
       "      <td>0.047884</td>\n",
       "      <td>-0.134075</td>\n",
       "      <td>9.782658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29 14:21:33.118321920</th>\n",
       "      <td>1.540823e+09</td>\n",
       "      <td>-0.214118</td>\n",
       "      <td>-0.024501</td>\n",
       "      <td>0.415453</td>\n",
       "      <td>-5.956743</td>\n",
       "      <td>-0.397435</td>\n",
       "      <td>7.086801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29 14:21:33.120321024</th>\n",
       "      <td>1.540823e+09</td>\n",
       "      <td>-0.238619</td>\n",
       "      <td>0.060720</td>\n",
       "      <td>0.476173</td>\n",
       "      <td>7.110743</td>\n",
       "      <td>1.402995</td>\n",
       "      <td>5.717324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29 14:21:33.122322944</th>\n",
       "      <td>1.540823e+09</td>\n",
       "      <td>-0.295078</td>\n",
       "      <td>0.106526</td>\n",
       "      <td>0.288687</td>\n",
       "      <td>-3.969566</td>\n",
       "      <td>-0.181958</td>\n",
       "      <td>16.136837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29 14:21:33.124325888</th>\n",
       "      <td>1.540823e+09</td>\n",
       "      <td>-0.078830</td>\n",
       "      <td>-0.073503</td>\n",
       "      <td>0.460194</td>\n",
       "      <td>-1.628049</td>\n",
       "      <td>0.090979</td>\n",
       "      <td>1.647202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-29 14:21:33.126321920</th>\n",
       "      <td>1.540823e+09</td>\n",
       "      <td>-0.348341</td>\n",
       "      <td>0.030893</td>\n",
       "      <td>0.392017</td>\n",
       "      <td>6.143490</td>\n",
       "      <td>2.159559</td>\n",
       "      <td>9.720408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38418 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  timestamp  ang_vel_x  ang_vel_y  ang_vel_z  \\\n",
       "timestamp                                                                      \n",
       "2018-10-29 14:20:16.292325120  1.540823e+09  -0.006392  -0.008522  -0.007457   \n",
       "2018-10-29 14:20:16.294326016  1.540823e+09  -0.005326  -0.006392  -0.006392   \n",
       "2018-10-29 14:20:16.296323072  1.540823e+09  -0.010653  -0.007457  -0.001065   \n",
       "2018-10-29 14:20:16.298322944  1.540823e+09  -0.009587  -0.009587  -0.002131   \n",
       "2018-10-29 14:20:16.300322048  1.540823e+09  -0.005326  -0.005326  -0.004261   \n",
       "...                                     ...        ...        ...        ...   \n",
       "2018-10-29 14:21:33.118321920  1.540823e+09  -0.214118  -0.024501   0.415453   \n",
       "2018-10-29 14:21:33.120321024  1.540823e+09  -0.238619   0.060720   0.476173   \n",
       "2018-10-29 14:21:33.122322944  1.540823e+09  -0.295078   0.106526   0.288687   \n",
       "2018-10-29 14:21:33.124325888  1.540823e+09  -0.078830  -0.073503   0.460194   \n",
       "2018-10-29 14:21:33.126321920  1.540823e+09  -0.348341   0.030893   0.392017   \n",
       "\n",
       "                               lin_acc_x  lin_acc_y  lin_acc_z  \n",
       "timestamp                                                       \n",
       "2018-10-29 14:20:16.292325120   0.052672  -0.158017   9.797023  \n",
       "2018-10-29 14:20:16.294326016   0.000000  -0.129286   9.768292  \n",
       "2018-10-29 14:20:16.296323072  -0.033519  -0.100556   9.749139  \n",
       "2018-10-29 14:20:16.298322944   0.043095  -0.110133   9.725197  \n",
       "2018-10-29 14:20:16.300322048   0.047884  -0.134075   9.782658  \n",
       "...                                  ...        ...        ...  \n",
       "2018-10-29 14:21:33.118321920  -5.956743  -0.397435   7.086801  \n",
       "2018-10-29 14:21:33.120321024   7.110743   1.402995   5.717324  \n",
       "2018-10-29 14:21:33.122322944  -3.969566  -0.181958  16.136837  \n",
       "2018-10-29 14:21:33.124325888  -1.628049   0.090979   1.647202  \n",
       "2018-10-29 14:21:33.126321920   6.143490   2.159559   9.720408  \n",
       "\n",
       "[38418 rows x 7 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"indoor_forward_9_snapdragon_with_gt/\"\n",
    "imu_cols=[\"timestamp\", \"ang_vel_x\", \"ang_vel_y\", \"ang_vel_z\", \"lin_acc_x\", \"lin_acc_y\", \"lin_acc_z\"]\n",
    "imu_df = pd.read_csv(data_dir + \"imu.txt\", delimiter=' ', skiprows=1, names=imu_cols)\n",
    "imu_df_nostamp = imu_df.loc[:, imu_df.columns != 'timestamp']\n",
    "imu_df.index = pd.to_datetime(imu_df[\"timestamp\"], unit='s')\n",
    "imu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.3916e-03, -8.5221e-03, -7.4569e-03,  5.2672e-02, -1.5802e-01,\n",
       "          9.7970e+00],\n",
       "        [-5.3263e-03, -6.3916e-03, -6.3916e-03,  0.0000e+00, -1.2929e-01,\n",
       "          9.7683e+00],\n",
       "        [-1.0653e-02, -7.4569e-03, -1.0653e-03, -3.3519e-02, -1.0056e-01,\n",
       "          9.7491e+00],\n",
       "        ...,\n",
       "        [-2.9508e-01,  1.0653e-01,  2.8869e-01, -3.9696e+00, -1.8196e-01,\n",
       "          1.6137e+01],\n",
       "        [-7.8830e-02, -7.3503e-02,  4.6019e-01, -1.6280e+00,  9.0979e-02,\n",
       "          1.6472e+00],\n",
       "        [-3.4834e-01,  3.0893e-02,  3.9202e-01,  6.1435e+00,  2.1596e+00,\n",
       "          9.7204e+00]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imu_data = imu_df_nostamp.to_numpy(dtype=np.float32)\n",
    "imu_data = torch.from_numpy(imu_data)\n",
    "imu_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4793, -0.7749,  1.4578,  0.9232, -1.1373, -0.9481],\n",
       "         [ 0.6458, -0.7473,  1.6251,  0.4890, -0.7620, -1.2505],\n",
       "         [ 0.7169, -0.5547,  1.3334,  0.8246, -1.0398, -1.2803],\n",
       "         [ 0.3889, -0.2781,  1.3305,  0.7418, -0.3609, -1.8223],\n",
       "         [ 0.5309, -1.1024,  1.6237,  0.6225, -0.6418, -1.0329],\n",
       "         [ 0.8339, -0.6135,  1.3578,  0.4818, -0.4256, -1.6344],\n",
       "         [ 0.8776, -0.4995,  1.1355,  0.8479, -0.8777, -1.4838],\n",
       "         [ 0.8451, -1.3519,  1.4210,  0.5744, -0.7680, -0.7206],\n",
       "         [ 0.8557, -0.5513,  1.3756,  0.6312, -1.1197, -1.1915],\n",
       "         [ 0.4572,  0.2107,  1.1180,  0.8811, -1.0470, -1.6199],\n",
       "         [ 0.4676, -1.5945,  1.2820,  0.9487, -0.3439, -0.7598],\n",
       "         [ 0.7939, -1.0143,  1.7428,  0.0138, -0.5200, -1.0163],\n",
       "         [ 0.6511, -0.7525,  1.5113,  0.4291, -0.2672, -1.5717],\n",
       "         [ 0.9575, -1.3649,  0.7857,  0.9521, -0.0283, -1.3021],\n",
       "         [ 0.4629, -0.4818,  1.3979,  0.8902, -0.7938, -1.4755],\n",
       "         [ 0.8163, -0.8062,  1.2004,  0.8245, -0.5214, -1.5136]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = imu_data[:ENC_SEQ_LEN].unsqueeze(0)\n",
    "transformer_encoder(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor([[ 0.0394, -0.1715, -0.1342, -1.8675, -0.9385,  7.2113]]),\n",
       "  tensor([ 6.9522e+00,  3.4244e+00, -9.4715e-01,  7.1251e-03, -5.9980e-03,\n",
       "           8.9050e-01, -4.5490e-01])),\n",
       " 14400)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class IMUDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_file, seq_len):\n",
    "        imu_cols=[\"timestamp\", \"ang_vel_x\", \"ang_vel_y\", \"ang_vel_z\", \"lin_acc_x\", \"lin_acc_y\", \"lin_acc_z\"]\n",
    "        self.imu_df = pd.read_csv(data_dir + \"imu.txt\", delimiter=' ', skiprows=1, names=imu_cols)\n",
    "        self.imu_df.index = pd.to_datetime(self.imu_df[\"timestamp\"], unit='s')\n",
    "        imu_df_nostamp = self.imu_df.loc[:, self.imu_df.columns != 'timestamp']\n",
    "        imu_data = imu_df_nostamp.to_numpy(dtype=np.float32)\n",
    "        self.imu_data = torch.from_numpy(imu_data)\n",
    "\n",
    "        label_cols=[\"timestamp\",\"tx\",\"ty\",\"tz\",\"qx\",\"qy\",\"qz\",\"qw\",\"none\"]\n",
    "        self.labels = pd.read_csv(label_file, delimiter=' ', skiprows=1, names=label_cols)\n",
    "        self.labels_df = self.labels.iloc[: , :-1]\n",
    "        self.labels_df[\"timestamp\"] = pd.to_datetime(self.labels_df[\"timestamp\"], unit='s')\n",
    "        labels_df_nostamp = self.labels_df.loc[:, self.labels_df.columns != 'timestamp']\n",
    "        self.labels = torch.tensor(labels_df_nostamp.to_numpy(), dtype=torch.float32)\n",
    "\n",
    "        # self.data_dir = data_dir\n",
    "        # self.transform = transforms.Compose([\n",
    "        #     transforms.Normalize(mean=[.0], std=[.25]),\n",
    "        # ])\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0] - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label_ts = self.labels_df.iloc[idx][\"timestamp\"]\n",
    "        # imu_index = self.imu_df.index.get_loc(label_ts, method='nearest')\n",
    "        # label = self.labels[idx]\n",
    "        # imu = self.imu_data[imu_index-self.seq_len:imu_index]\n",
    "\n",
    "        label_ts_end = self.labels_df.iloc[idx+1][\"timestamp\"]\n",
    "\n",
    "        imu_index_start = self.imu_df.index.get_loc(label_ts, method='nearest')\n",
    "        imu_index_end = self.imu_df.index.get_loc(label_ts_end, method='nearest')\n",
    "        imu = self.imu_data[imu_index_start:imu_index_end]\n",
    "\n",
    "        return imu, label\n",
    "ENC_SEQ_LEN = 1\n",
    "vio_dataset = IMUDataset(data_dir, data_dir + \"groundtruth.txt\", ENC_SEQ_LEN)\n",
    "vio_dataset.__getitem__(0), len(vio_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "ENC_SEQ_LEN = 32\n",
    "vio_dataset = IMUDataset(data_dir, data_dir + \"groundtruth.txt\", ENC_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 6])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_point = vio_dataset.__getitem__(0)[0].unsqueeze(0)\n",
    "transformer_encoder(data_point).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IMUTransformer(\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=6, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=6, bias=True)\n",
       "        (norm1): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=192, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class IMUTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=6, nhead=3, batch_first=True)\n",
    "        transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        self.encoder = transformer_encoder\n",
    "        self.fc1 = nn.Linear(in_features=6*ENC_SEQ_LEN, out_features=7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.encoder(x))\n",
    "        # x = self.fc1(x[:,-1,:])\n",
    "        x = self.fc1(x.flatten(start_dim=1))\n",
    "        return x\n",
    "\n",
    "model = IMUTransformer()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0589, -0.2881,  0.2586,  0.5160, -0.0012,  0.3281,  0.1028]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32, 6])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "dataloader = torch.utils.data.DataLoader(vio_dataset, batch_size=16, shuffle=True, num_workers=8)\n",
    "next(iter(dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "35.52125549316406\n",
      "25.786216735839844\n",
      "23.18031883239746\n",
      "22.320993423461914\n",
      "22.12074851989746\n",
      "17.539819717407227\n",
      "18.301855087280273\n",
      "13.122197151184082\n",
      "12.12604808807373\n",
      "19.72570037841797\n",
      "29.761049270629883\n",
      "19.9506893157959\n",
      "21.220523834228516\n",
      "25.01456642150879\n",
      "20.1151123046875\n",
      "20.03872299194336\n",
      "27.475521087646484\n",
      "26.811931610107422\n",
      "9.079036712646484\n",
      "24.223478317260742\n",
      "23.523714065551758\n",
      "23.13140106201172\n",
      "16.113121032714844\n",
      "25.238595962524414\n",
      "14.324413299560547\n",
      "14.139582633972168\n",
      "17.625995635986328\n",
      "12.076887130737305\n",
      "16.461118698120117\n",
      "12.688921928405762\n",
      "12.94564437866211\n",
      "16.034595489501953\n",
      "7.057498931884766\n",
      "12.859416961669922\n",
      "13.376384735107422\n",
      "8.927509307861328\n",
      "18.10009765625\n",
      "12.959074020385742\n",
      "15.27052116394043\n",
      "19.59908676147461\n",
      "11.718914985656738\n",
      "15.963224411010742\n",
      "11.39657211303711\n",
      "14.265795707702637\n",
      "14.385345458984375\n",
      "11.069875717163086\n",
      "9.015377044677734\n",
      "16.388229370117188\n",
      "12.739788055419922\n",
      "12.998178482055664\n",
      "18.923484802246094\n",
      "12.6312837600708\n",
      "11.483369827270508\n",
      "11.990998268127441\n",
      "10.655070304870605\n",
      "10.658271789550781\n",
      "7.950179100036621\n",
      "11.774584770202637\n",
      "8.28982162475586\n",
      "7.298760414123535\n",
      "12.429984092712402\n",
      "9.592434883117676\n",
      "7.693398475646973\n",
      "14.038777351379395\n",
      "8.997733116149902\n",
      "13.120518684387207\n",
      "7.809647083282471\n",
      "10.477054595947266\n",
      "6.872124195098877\n",
      "11.114527702331543\n",
      "10.620935440063477\n",
      "6.001489162445068\n",
      "7.434736251831055\n",
      "4.349274635314941\n",
      "10.764095306396484\n",
      "9.698415756225586\n",
      "7.285095691680908\n",
      "9.033098220825195\n",
      "11.446161270141602\n",
      "8.979674339294434\n",
      "10.719194412231445\n",
      "9.185302734375\n",
      "7.4396233558654785\n",
      "8.123294830322266\n",
      "10.819147109985352\n",
      "8.925135612487793\n",
      "8.473424911499023\n",
      "9.014963150024414\n",
      "9.413708686828613\n",
      "8.915350914001465\n",
      "6.426402568817139\n",
      "7.403787136077881\n",
      "8.340001106262207\n",
      "8.57495403289795\n",
      "9.917387008666992\n",
      "6.070236682891846\n",
      "10.278365135192871\n",
      "8.620465278625488\n",
      "10.815571784973145\n",
      "6.104809761047363\n",
      "9.574179649353027\n",
      "8.611703872680664\n",
      "10.236655235290527\n",
      "5.372908115386963\n",
      "7.512239456176758\n",
      "7.581216335296631\n",
      "5.724852561950684\n",
      "4.2211833000183105\n",
      "6.6060261726379395\n",
      "9.023943901062012\n",
      "5.3365349769592285\n",
      "8.467272758483887\n",
      "6.892720699310303\n",
      "5.048539161682129\n",
      "4.700799942016602\n",
      "7.3302717208862305\n",
      "8.145208358764648\n",
      "5.440615653991699\n",
      "5.894190788269043\n",
      "11.362194061279297\n",
      "6.611706733703613\n",
      "6.321920394897461\n",
      "5.882061958312988\n",
      "8.725936889648438\n",
      "7.089130878448486\n",
      "4.736943244934082\n",
      "9.063284873962402\n",
      "12.021660804748535\n",
      "9.449244499206543\n",
      "11.904886245727539\n",
      "10.092514038085938\n",
      "7.4295654296875\n",
      "6.716274738311768\n",
      "8.20178508758545\n",
      "7.114091396331787\n",
      "7.190868377685547\n",
      "7.350320816040039\n",
      "6.7380757331848145\n",
      "8.034652709960938\n",
      "7.617623805999756\n",
      "7.974790096282959\n",
      "6.364078044891357\n",
      "10.334186553955078\n",
      "6.416784286499023\n",
      "12.665396690368652\n",
      "8.289379119873047\n",
      "5.620538234710693\n",
      "8.257244110107422\n",
      "7.7221150398254395\n",
      "8.55576229095459\n",
      "7.7234907150268555\n",
      "4.922896862030029\n",
      "6.430354595184326\n",
      "7.323071002960205\n",
      "7.350771903991699\n",
      "5.981784343719482\n",
      "6.482086658477783\n",
      "7.174379825592041\n",
      "10.194162368774414\n",
      "6.31934928894043\n",
      "7.114343643188477\n",
      "7.411855220794678\n",
      "7.293166160583496\n",
      "5.890227317810059\n",
      "5.948861598968506\n",
      "5.826272964477539\n",
      "6.48146390914917\n",
      "6.25615930557251\n",
      "7.026798248291016\n",
      "5.847959041595459\n",
      "7.570019721984863\n",
      "8.664545059204102\n",
      "7.138460159301758\n",
      "6.304492473602295\n",
      "7.983773708343506\n",
      "5.8288960456848145\n",
      "7.298637866973877\n",
      "7.991180896759033\n",
      "8.720587730407715\n",
      "9.064985275268555\n",
      "6.769771099090576\n",
      "4.971208572387695\n",
      "9.321744918823242\n",
      "6.215211391448975\n",
      "6.81684684753418\n",
      "4.796887397766113\n",
      "7.086506366729736\n",
      "6.986874103546143\n",
      "6.559484958648682\n",
      "5.157094955444336\n",
      "10.564166069030762\n",
      "8.068916320800781\n",
      "6.553266525268555\n",
      "6.9626784324646\n",
      "7.126832008361816\n",
      "7.394845962524414\n",
      "8.071341514587402\n",
      "8.686111450195312\n",
      "8.226725578308105\n",
      "7.364010810852051\n",
      "7.375882148742676\n",
      "8.961333274841309\n",
      "7.903109073638916\n",
      "9.637266159057617\n",
      "7.230828762054443\n",
      "8.873719215393066\n",
      "8.021268844604492\n",
      "7.408635139465332\n",
      "6.749205112457275\n",
      "7.8132429122924805\n",
      "8.245332717895508\n",
      "8.690718650817871\n",
      "7.440586090087891\n",
      "7.576048374176025\n",
      "7.681128025054932\n",
      "8.14903736114502\n",
      "6.9064836502075195\n",
      "4.641733169555664\n",
      "10.028804779052734\n",
      "9.482136726379395\n",
      "5.316035270690918\n",
      "5.085513591766357\n",
      "8.171469688415527\n",
      "8.236449241638184\n",
      "7.395015716552734\n",
      "7.78183126449585\n",
      "8.211333274841309\n",
      "6.6694655418396\n",
      "8.021594047546387\n",
      "7.654150485992432\n",
      "5.213613510131836\n",
      "7.620526313781738\n",
      "9.446736335754395\n",
      "6.642493724822998\n",
      "7.037756443023682\n",
      "7.045075416564941\n",
      "6.307417869567871\n",
      "7.746950626373291\n",
      "6.684264183044434\n",
      "7.137752532958984\n",
      "5.9738240242004395\n",
      "8.104324340820312\n",
      "8.363020896911621\n",
      "7.748500823974609\n",
      "5.252431392669678\n",
      "6.934138298034668\n",
      "7.155876636505127\n",
      "6.052332401275635\n",
      "8.277740478515625\n",
      "12.182811737060547\n",
      "8.893671989440918\n",
      "9.09946346282959\n",
      "7.412355899810791\n",
      "8.40610122680664\n",
      "7.163720607757568\n",
      "7.972777366638184\n",
      "6.963433742523193\n",
      "7.119356632232666\n",
      "4.486167907714844\n",
      "8.604227066040039\n",
      "8.896211624145508\n",
      "6.4721479415893555\n",
      "7.538153648376465\n",
      "9.529308319091797\n",
      "10.770760536193848\n",
      "7.255070209503174\n",
      "6.071898937225342\n",
      "7.184567451477051\n",
      "7.8467116355896\n",
      "7.308615684509277\n",
      "5.925759792327881\n",
      "6.019109725952148\n",
      "5.714885234832764\n",
      "7.844970703125\n",
      "8.728053092956543\n",
      "6.360874652862549\n",
      "6.206318378448486\n",
      "9.10371208190918\n",
      "8.244800567626953\n",
      "7.169710636138916\n",
      "7.1270527839660645\n",
      "10.206385612487793\n",
      "7.145116329193115\n",
      "8.672452926635742\n",
      "5.281797409057617\n",
      "5.837923049926758\n",
      "7.691634178161621\n",
      "6.65071964263916\n",
      "10.379302978515625\n",
      "5.466395854949951\n",
      "7.935849189758301\n",
      "8.402421951293945\n",
      "6.003126621246338\n",
      "7.929643630981445\n",
      "10.418872833251953\n",
      "8.283919334411621\n",
      "7.380045413970947\n",
      "7.58692741394043\n",
      "6.435774326324463\n",
      "6.123418807983398\n",
      "6.231147289276123\n",
      "8.205747604370117\n",
      "5.781439781188965\n",
      "7.941397666931152\n",
      "6.2532854080200195\n",
      "8.341556549072266\n",
      "5.516873359680176\n",
      "7.931746482849121\n",
      "7.10922384262085\n",
      "7.985915184020996\n",
      "6.714212894439697\n",
      "6.85310697555542\n",
      "7.591496467590332\n",
      "9.549674034118652\n",
      "7.7402663230896\n",
      "6.188672065734863\n",
      "6.598770618438721\n",
      "7.064138412475586\n",
      "6.7749552726745605\n",
      "6.545358180999756\n",
      "7.4625935554504395\n",
      "6.946728706359863\n",
      "5.407851219177246\n",
      "4.95701789855957\n",
      "8.467806816101074\n",
      "7.450247764587402\n",
      "6.264937877655029\n",
      "8.06010913848877\n",
      "6.463202476501465\n",
      "8.010002136230469\n",
      "8.918572425842285\n",
      "8.048337936401367\n",
      "6.133147239685059\n",
      "6.728341579437256\n",
      "7.2696309089660645\n",
      "7.421860694885254\n",
      "5.4546217918396\n",
      "8.73361873626709\n",
      "8.009334564208984\n",
      "6.965076446533203\n",
      "6.9261794090271\n",
      "5.291450023651123\n",
      "5.9548749923706055\n",
      "7.845965385437012\n",
      "7.05454158782959\n",
      "5.8684983253479\n",
      "5.712097644805908\n",
      "6.701766014099121\n",
      "6.107457160949707\n",
      "5.839991569519043\n",
      "5.28391170501709\n",
      "8.699752807617188\n",
      "6.38891077041626\n",
      "6.911850452423096\n",
      "9.608870506286621\n",
      "7.5214128494262695\n",
      "7.232723712921143\n",
      "8.600696563720703\n",
      "6.942668914794922\n",
      "5.037951946258545\n",
      "10.27015209197998\n",
      "7.4836626052856445\n",
      "7.030346393585205\n",
      "8.004838943481445\n",
      "8.271913528442383\n",
      "10.42880916595459\n",
      "7.999715328216553\n",
      "5.097389221191406\n",
      "7.185131072998047\n",
      "4.6367058753967285\n",
      "10.478940963745117\n",
      "8.000655174255371\n",
      "9.463712692260742\n",
      "11.020752906799316\n",
      "7.573423385620117\n",
      "6.494858741760254\n",
      "5.982827186584473\n",
      "3.681777238845825\n",
      "8.476018905639648\n",
      "5.266570091247559\n",
      "6.221035480499268\n",
      "6.742110252380371\n",
      "9.04807186126709\n",
      "4.465668678283691\n",
      "5.54530668258667\n",
      "6.313281536102295\n",
      "7.710028648376465\n",
      "6.398081302642822\n",
      "6.5740437507629395\n",
      "4.714907169342041\n",
      "7.524960041046143\n",
      "8.381134033203125\n",
      "6.066129684448242\n",
      "5.542283535003662\n",
      "6.959051609039307\n",
      "8.149580001831055\n",
      "6.981980800628662\n",
      "7.046947479248047\n",
      "7.274661064147949\n",
      "9.243925094604492\n",
      "5.456669330596924\n",
      "6.747285842895508\n",
      "5.697327136993408\n",
      "7.662461280822754\n",
      "8.682031631469727\n",
      "9.240434646606445\n",
      "7.741141319274902\n",
      "8.199581146240234\n",
      "6.725527763366699\n",
      "8.1422119140625\n",
      "7.84335994720459\n",
      "6.93134880065918\n",
      "7.096958160400391\n",
      "6.755806922912598\n",
      "7.197038173675537\n",
      "7.7376275062561035\n",
      "6.241616725921631\n",
      "7.835331916809082\n",
      "7.307013511657715\n",
      "7.343794822692871\n",
      "7.064609527587891\n",
      "6.357411861419678\n",
      "6.718631267547607\n",
      "6.018664836883545\n",
      "6.709965705871582\n",
      "6.923361301422119\n",
      "5.441013813018799\n",
      "5.725762844085693\n",
      "7.345765590667725\n",
      "7.547381401062012\n",
      "5.173840522766113\n",
      "7.04194450378418\n",
      "4.676811218261719\n",
      "5.855868816375732\n",
      "7.6583075523376465\n",
      "9.208966255187988\n",
      "7.787331581115723\n",
      "6.8704118728637695\n",
      "10.072477340698242\n",
      "7.642929553985596\n",
      "8.19965934753418\n",
      "4.982629299163818\n",
      "6.111799240112305\n",
      "7.547639846801758\n",
      "5.826870918273926\n",
      "7.853429794311523\n",
      "7.192974090576172\n",
      "6.3663249015808105\n",
      "8.285310745239258\n",
      "6.320300102233887\n",
      "3.754751682281494\n",
      "8.131031036376953\n",
      "8.463175773620605\n",
      "9.897555351257324\n",
      "6.063273906707764\n",
      "5.418368816375732\n",
      "6.827545642852783\n",
      "9.406476974487305\n",
      "9.256232261657715\n",
      "7.709774971008301\n",
      "8.065970420837402\n",
      "7.36353874206543\n",
      "6.311257839202881\n",
      "9.277745246887207\n",
      "10.342869758605957\n",
      "9.518461227416992\n",
      "9.227579116821289\n",
      "5.546187400817871\n",
      "6.399377822875977\n",
      "6.543133735656738\n",
      "5.199357986450195\n",
      "9.019503593444824\n",
      "8.134990692138672\n",
      "9.344138145446777\n",
      "6.570356369018555\n",
      "7.083094120025635\n",
      "7.051152229309082\n",
      "7.681360721588135\n",
      "7.948963165283203\n",
      "9.44823169708252\n",
      "5.312370300292969\n",
      "7.876814365386963\n",
      "5.829133987426758\n",
      "5.428094863891602\n",
      "6.06057071685791\n",
      "6.972680568695068\n",
      "7.7860426902771\n",
      "8.575681686401367\n",
      "6.897592067718506\n",
      "7.86247444152832\n",
      "5.916810035705566\n",
      "8.02515983581543\n",
      "7.81311559677124\n",
      "11.298105239868164\n",
      "7.926506519317627\n",
      "6.994597434997559\n",
      "5.533163070678711\n",
      "6.231058597564697\n",
      "11.869701385498047\n",
      "8.993013381958008\n",
      "6.596970081329346\n",
      "6.785241603851318\n",
      "9.541013717651367\n",
      "7.156345844268799\n",
      "7.522637844085693\n",
      "8.667116165161133\n",
      "8.912115097045898\n",
      "6.976552486419678\n",
      "9.91081428527832\n",
      "8.941237449645996\n",
      "6.623571395874023\n",
      "6.170579433441162\n",
      "8.06005859375\n",
      "8.311429023742676\n",
      "5.839567184448242\n",
      "7.2059407234191895\n",
      "6.096200942993164\n",
      "6.949010848999023\n",
      "7.332518577575684\n",
      "9.084222793579102\n",
      "7.442750453948975\n",
      "8.252098083496094\n",
      "7.667862415313721\n",
      "6.10654878616333\n",
      "11.724638938903809\n",
      "7.3173675537109375\n",
      "8.109254837036133\n",
      "8.76280403137207\n",
      "6.877793312072754\n",
      "5.712438583374023\n",
      "5.015561580657959\n",
      "6.430490016937256\n",
      "7.010095119476318\n",
      "5.922452449798584\n",
      "7.028397083282471\n",
      "6.362051963806152\n",
      "7.670153617858887\n",
      "6.1436543464660645\n",
      "6.7520833015441895\n",
      "8.252616882324219\n",
      "7.234037399291992\n",
      "9.484909057617188\n",
      "6.7164506912231445\n",
      "8.439132690429688\n",
      "7.431403160095215\n",
      "6.737747669219971\n",
      "5.948203086853027\n",
      "7.79921817779541\n",
      "10.130282402038574\n",
      "7.3597517013549805\n",
      "5.667715072631836\n",
      "7.914795875549316\n",
      "5.392385005950928\n",
      "7.832500457763672\n",
      "10.094356536865234\n",
      "8.027266502380371\n",
      "8.222795486450195\n",
      "8.31566333770752\n",
      "9.75714111328125\n",
      "5.2674880027771\n",
      "5.920653343200684\n",
      "7.878196716308594\n",
      "7.294984340667725\n",
      "6.152585029602051\n",
      "5.419938564300537\n",
      "9.237518310546875\n",
      "6.9665093421936035\n",
      "7.377216815948486\n",
      "9.243894577026367\n",
      "7.455082416534424\n",
      "9.253829956054688\n",
      "9.919015884399414\n",
      "7.546770095825195\n",
      "8.46101188659668\n",
      "8.193206787109375\n",
      "7.535050392150879\n",
      "6.017110347747803\n",
      "7.670252323150635\n",
      "6.703486442565918\n",
      "7.210898399353027\n",
      "7.216366767883301\n",
      "9.371047973632812\n",
      "9.052138328552246\n",
      "7.794881820678711\n",
      "6.932581424713135\n",
      "7.4080586433410645\n",
      "6.790124893188477\n",
      "10.230029106140137\n",
      "2.5805726051330566\n",
      "9.52252197265625\n",
      "6.299818992614746\n",
      "7.646417140960693\n",
      "7.494015216827393\n",
      "7.384886741638184\n",
      "5.585167407989502\n",
      "8.201072692871094\n",
      "8.073943138122559\n",
      "8.459081649780273\n",
      "7.834371566772461\n",
      "8.504816055297852\n",
      "7.906968593597412\n",
      "9.931258201599121\n",
      "6.3767828941345215\n",
      "7.21920919418335\n",
      "7.519161224365234\n",
      "7.859586238861084\n",
      "8.205702781677246\n",
      "8.395477294921875\n",
      "6.156560897827148\n",
      "7.405073642730713\n",
      "8.527485847473145\n",
      "6.2550458908081055\n",
      "7.765659809112549\n",
      "6.224233627319336\n",
      "7.426262855529785\n",
      "5.887557029724121\n",
      "7.196138381958008\n",
      "7.628096580505371\n",
      "7.608418941497803\n",
      "8.633054733276367\n",
      "8.266536712646484\n",
      "6.104274272918701\n",
      "6.800382137298584\n",
      "6.57942008972168\n",
      "8.030790328979492\n",
      "8.813875198364258\n",
      "5.914093494415283\n",
      "6.989361763000488\n",
      "8.059935569763184\n",
      "10.49407958984375\n",
      "6.952425479888916\n",
      "7.097763538360596\n",
      "8.14102554321289\n",
      "7.154226303100586\n",
      "6.699551582336426\n",
      "7.690934658050537\n",
      "7.17353630065918\n",
      "8.654470443725586\n",
      "7.119138717651367\n",
      "7.687727451324463\n",
      "8.34701919555664\n",
      "6.1097893714904785\n",
      "6.060276985168457\n",
      "7.494131088256836\n",
      "7.663816452026367\n",
      "6.1429619789123535\n",
      "5.9509477615356445\n",
      "6.9314141273498535\n",
      "7.557855606079102\n",
      "6.234076023101807\n",
      "8.679896354675293\n",
      "5.029988765716553\n",
      "5.981518745422363\n",
      "10.217069625854492\n",
      "6.825476169586182\n",
      "6.693045139312744\n",
      "6.085553169250488\n",
      "6.443435192108154\n",
      "7.17577600479126\n",
      "5.901773929595947\n",
      "8.996332168579102\n",
      "5.805984973907471\n",
      "5.670501232147217\n",
      "5.944514751434326\n",
      "5.822935104370117\n",
      "7.485240936279297\n",
      "6.70137357711792\n",
      "8.005050659179688\n",
      "6.134056568145752\n",
      "6.870552062988281\n",
      "8.047718048095703\n",
      "11.267931938171387\n",
      "7.549656391143799\n",
      "5.964188575744629\n",
      "7.499037742614746\n",
      "6.999542236328125\n",
      "5.220324993133545\n",
      "6.6460041999816895\n",
      "5.4009294509887695\n",
      "5.157391548156738\n",
      "6.413407802581787\n",
      "6.10377836227417\n",
      "4.95653772354126\n",
      "5.856841087341309\n",
      "5.346020698547363\n",
      "7.396779537200928\n",
      "5.40315055847168\n",
      "6.473754405975342\n",
      "6.983304023742676\n",
      "9.565774917602539\n",
      "9.879829406738281\n",
      "9.888195991516113\n",
      "8.115738868713379\n",
      "5.35935115814209\n",
      "8.20350456237793\n",
      "7.0289387702941895\n",
      "6.228798866271973\n",
      "6.645062446594238\n",
      "6.112195014953613\n",
      "6.979338645935059\n",
      "5.6852288246154785\n",
      "7.34397554397583\n",
      "5.71442174911499\n",
      "5.292716979980469\n",
      "5.980340003967285\n",
      "5.288898468017578\n",
      "7.648242950439453\n",
      "7.8618364334106445\n",
      "8.345499038696289\n",
      "7.461618423461914\n",
      "7.153256416320801\n",
      "7.217113494873047\n",
      "9.149934768676758\n",
      "7.6709160804748535\n",
      "6.889864444732666\n",
      "8.333601951599121\n",
      "5.402449131011963\n",
      "7.230775356292725\n",
      "6.844386577606201\n",
      "7.915473937988281\n",
      "8.024556159973145\n",
      "6.499639511108398\n",
      "6.5206685066223145\n",
      "5.446671962738037\n",
      "8.44239616394043\n",
      "4.682666301727295\n",
      "7.284709930419922\n",
      "7.678845405578613\n",
      "7.384365558624268\n",
      "6.923341274261475\n",
      "7.611198902130127\n",
      "8.02340030670166\n",
      "7.445528507232666\n",
      "11.2634859085083\n",
      "5.686732292175293\n",
      "8.658132553100586\n",
      "4.588610649108887\n",
      "8.902576446533203\n",
      "7.438981533050537\n",
      "7.677215576171875\n",
      "6.45405912399292\n",
      "7.442857265472412\n",
      "6.789931297302246\n",
      "7.55091667175293\n",
      "9.275117874145508\n",
      "7.280791282653809\n",
      "7.295565605163574\n",
      "9.676817893981934\n",
      "8.28950309753418\n",
      "8.93647575378418\n",
      "6.463624954223633\n",
      "8.293112754821777\n",
      "8.838768005371094\n",
      "8.271526336669922\n",
      "7.78447961807251\n",
      "8.883227348327637\n",
      "6.098599433898926\n",
      "6.908720970153809\n",
      "7.602502346038818\n",
      "6.909623146057129\n",
      "6.551420211791992\n",
      "5.667663097381592\n",
      "5.510084629058838\n",
      "7.063135623931885\n",
      "7.269506454467773\n",
      "8.858335494995117\n",
      "6.362559795379639\n",
      "5.859442710876465\n",
      "5.596119403839111\n",
      "6.249770164489746\n",
      "6.376288890838623\n",
      "7.358456611633301\n",
      "10.599120140075684\n",
      "5.986713886260986\n",
      "5.2882232666015625\n",
      "8.177982330322266\n",
      "7.880259037017822\n",
      "10.384419441223145\n",
      "7.9603776931762695\n",
      "7.422940254211426\n",
      "5.685617923736572\n",
      "6.7624897956848145\n",
      "8.292724609375\n",
      "7.385744094848633\n",
      "6.253483295440674\n",
      "8.360868453979492\n",
      "7.180923938751221\n",
      "7.1075029373168945\n",
      "6.539148807525635\n",
      "6.664600849151611\n",
      "7.255985736846924\n",
      "7.491128921508789\n",
      "6.986428737640381\n",
      "10.775836944580078\n",
      "6.751542568206787\n",
      "6.932443141937256\n",
      "8.612364768981934\n",
      "10.33004379272461\n",
      "7.019536972045898\n",
      "4.502556800842285\n",
      "8.662817001342773\n",
      "8.34367847442627\n",
      "7.5933027267456055\n",
      "6.4513630867004395\n",
      "9.385965347290039\n",
      "5.867376327514648\n",
      "7.915964603424072\n",
      "7.736845970153809\n",
      "8.26620864868164\n",
      "6.687474727630615\n",
      "6.9167070388793945\n",
      "8.106097221374512\n",
      "6.3949480056762695\n",
      "8.084620475769043\n",
      "9.143735885620117\n",
      "7.095465660095215\n",
      "7.9451985359191895\n",
      "6.844881534576416\n",
      "6.0369744300842285\n",
      "7.91799783706665\n",
      "8.567436218261719\n",
      "6.59799337387085\n",
      "5.1984453201293945\n",
      "6.045263290405273\n",
      "10.552166938781738\n",
      "5.745478630065918\n",
      "5.863901615142822\n",
      "10.520590782165527\n",
      "7.069798469543457\n",
      "5.668398380279541\n",
      "5.776342391967773\n",
      "4.701498985290527\n",
      "7.45175313949585\n",
      "6.619060039520264\n",
      "7.240072250366211\n",
      "7.701419830322266\n",
      "5.704594612121582\n",
      "5.651880264282227\n",
      "8.697715759277344\n",
      "7.406000137329102\n",
      "9.225058555603027\n",
      "6.690802574157715\n",
      "6.239175319671631\n",
      "7.9903564453125\n",
      "6.955870151519775\n",
      "6.325899600982666\n",
      "6.49317741394043\n",
      "6.08521842956543\n",
      "4.970449447631836\n",
      "7.329412937164307\n",
      "8.986907958984375\n",
      "9.91064739227295\n",
      "6.819206714630127\n",
      "7.503076553344727\n",
      "6.40336275100708\n",
      "8.55724811553955\n",
      "6.44219446182251\n",
      "6.549111843109131\n",
      "7.240604400634766\n",
      "6.9756927490234375\n",
      "7.833442687988281\n",
      "8.862733840942383\n",
      "7.699804782867432\n",
      "4.775617599487305\n",
      "7.6539387702941895\n",
      "6.863015174865723\n",
      "7.653329372406006\n",
      "5.708882808685303\n",
      "7.725964069366455\n",
      "6.812267780303955\n",
      "8.929899215698242\n",
      "9.12919807434082\n",
      "7.278599739074707\n",
      "5.978151798248291\n",
      "5.547647953033447\n",
      "7.785989284515381\n",
      "8.740592956542969\n",
      "6.90593147277832\n",
      "9.444676399230957\n",
      "8.261320114135742\n",
      "7.113637924194336\n",
      "4.619133949279785\n",
      "5.680150032043457\n",
      "7.714292526245117\n",
      "5.262162685394287\n",
      "5.652562141418457\n",
      "6.442842483520508\n",
      "6.456920146942139\n",
      "4.853733062744141\n",
      "6.613474369049072\n",
      "9.849896430969238\n",
      "8.465847969055176\n",
      "7.4340105056762695\n",
      "7.675753593444824\n",
      "8.300471305847168\n",
      "7.143651485443115\n",
      "7.6137375831604\n",
      "7.386656761169434\n",
      "6.294686794281006\n",
      "5.401440620422363\n",
      "train Loss: 8.0082\n",
      "Epoch 1/9\n",
      "----------\n",
      "6.666638374328613\n",
      "6.906099319458008\n",
      "8.050719261169434\n",
      "8.126358032226562\n",
      "8.370494842529297\n",
      "7.2410054206848145\n",
      "6.259787082672119\n",
      "6.209266185760498\n",
      "9.147452354431152\n",
      "7.6014204025268555\n",
      "6.885063648223877\n",
      "6.519133567810059\n",
      "9.539777755737305\n",
      "6.330463886260986\n",
      "5.843568325042725\n",
      "7.7743425369262695\n",
      "6.7478251457214355\n",
      "5.914700984954834\n",
      "7.6717634201049805\n",
      "7.213204383850098\n",
      "5.89417839050293\n",
      "7.0006489753723145\n",
      "8.088598251342773\n",
      "6.139078617095947\n",
      "9.72018051147461\n",
      "10.258976936340332\n",
      "6.681455612182617\n",
      "10.017603874206543\n",
      "6.531805038452148\n",
      "6.317845821380615\n",
      "5.83761739730835\n",
      "6.959938049316406\n",
      "9.93353271484375\n",
      "6.896101951599121\n",
      "7.196232795715332\n",
      "9.389959335327148\n",
      "5.350518703460693\n",
      "7.321346282958984\n",
      "5.950201988220215\n",
      "6.314377307891846\n",
      "6.064129829406738\n",
      "4.279160976409912\n",
      "5.070723533630371\n",
      "8.423748016357422\n",
      "5.9130330085754395\n",
      "6.837906360626221\n",
      "7.895650863647461\n",
      "8.360793113708496\n",
      "6.722458362579346\n",
      "7.581055164337158\n",
      "8.111169815063477\n",
      "7.4712605476379395\n",
      "7.851459503173828\n",
      "5.532077312469482\n",
      "6.109731674194336\n",
      "8.083885192871094\n",
      "8.658961296081543\n",
      "6.196572780609131\n",
      "5.941202163696289\n",
      "10.171191215515137\n",
      "6.736973762512207\n",
      "9.324856758117676\n",
      "5.99970817565918\n",
      "8.405384063720703\n",
      "5.933152675628662\n",
      "6.811452865600586\n",
      "7.0157790184021\n",
      "8.195426940917969\n",
      "6.961945056915283\n",
      "8.712530136108398\n",
      "7.9331135749816895\n",
      "6.135985374450684\n",
      "3.4290409088134766\n",
      "7.44267463684082\n",
      "7.054081439971924\n",
      "7.949855804443359\n",
      "8.454283714294434\n",
      "13.013147354125977\n",
      "6.235276699066162\n",
      "6.884853363037109\n",
      "5.81611967086792\n",
      "8.562427520751953\n",
      "7.897438049316406\n",
      "6.7697319984436035\n",
      "8.188394546508789\n",
      "7.023521900177002\n",
      "8.130182266235352\n",
      "6.933080196380615\n",
      "6.394455909729004\n",
      "7.140480041503906\n",
      "6.489511013031006\n",
      "7.601346492767334\n",
      "7.084985256195068\n",
      "7.92113733291626\n",
      "10.289308547973633\n",
      "6.906792640686035\n",
      "7.415485382080078\n",
      "8.043489456176758\n",
      "6.140483379364014\n",
      "6.901651859283447\n",
      "7.28241491317749\n",
      "9.501705169677734\n",
      "7.009466171264648\n",
      "7.176043510437012\n",
      "7.221309661865234\n",
      "8.019160270690918\n",
      "7.804361820220947\n",
      "7.703936576843262\n",
      "6.263241767883301\n",
      "4.906911849975586\n",
      "6.25889253616333\n",
      "7.518709182739258\n",
      "6.895966529846191\n",
      "6.371769905090332\n",
      "7.129213809967041\n",
      "9.091769218444824\n",
      "6.515057563781738\n",
      "7.805522918701172\n",
      "9.127997398376465\n",
      "7.707348346710205\n",
      "6.561578273773193\n",
      "7.473515510559082\n",
      "8.768362998962402\n",
      "8.561332702636719\n",
      "8.030559539794922\n",
      "8.049115180969238\n",
      "7.866940498352051\n",
      "9.777280807495117\n",
      "10.100348472595215\n",
      "8.387234687805176\n",
      "5.948846340179443\n",
      "8.174375534057617\n",
      "8.697524070739746\n",
      "8.255144119262695\n",
      "6.490306377410889\n",
      "7.086699485778809\n",
      "7.5400471687316895\n",
      "6.298266887664795\n",
      "7.233247756958008\n",
      "7.722924709320068\n",
      "9.426688194274902\n",
      "9.399321556091309\n",
      "5.2249040603637695\n",
      "9.148849487304688\n",
      "5.063277721405029\n",
      "7.7452778816223145\n",
      "7.948558807373047\n",
      "6.3218255043029785\n",
      "9.624116897583008\n",
      "6.62593412399292\n",
      "7.697328090667725\n",
      "6.994790077209473\n",
      "9.604827880859375\n",
      "8.20321273803711\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21523/881039961.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    169\u001b[0m                  \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                  \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'foreach'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                  capturable=group['capturable'])\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    224\u001b[0m          \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m          \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m          capturable=capturable)\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcapturable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html training loop from here\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    for phase in ['train']:\n",
    "        if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "        else:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over data.\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            # labels =  torch.ones_like(labels).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(vio_dataset)\n",
    "\n",
    "        print(f'{phase} Loss: {epoch_loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
